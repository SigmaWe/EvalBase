# An evaluation framework for text-to-text generation tasks 

EvalBase allow easy testing of new text-to-text generation metrics. A computerprogram or a machine learning model that can generate text from text is called a **system**. Such text-to-text generation tasks can be __summarization__, __translation__, or even __question answering__. Correspondingly, the systems for such tasks are called **summarizers**, **translators**, and **answer**. 

There are usually two approaches to text-to-text generation: reference-based and reference-free. For each sample in a test set, a **reference-based** approach first let a human do the same task to produce a **reference** and then compares a machine/system-generated output against the reference, while **reference-free** approaches directly scores the generated text with respect to the input text. 

## Usage 

EvalBase works with both reference-free and reference-based metrics. 
It automatically evaluates metrics specified in the configuration file `env.py` on several human evaluation datasets for summarization. As long as the files are in respected locations, you don't have to worry about loading the test data and getting human ratings from them. 

### Getting and preparing dataset files
* SummEval: Go to `dataloader` and run `summeval_build.sh`
* Realsumm: Go to `dataloader` and run `realsumm_build.sh` 
* Newsroom: Go to `dataloader` and do the follows. 
  - Download `newsroom-human-eval.csv`, the human evaluation result, including documents and system summaries but no reference summaries:
    ```shell
    wget https://github.com/lil-lab/newsroom/raw/master/humaneval/newsroom-human-eval.csv
    ```
  - Get `test.jsonl`, the test split of Newsroom, containing reference summaries. No automatic script. You will have to fill out a web form [here](https://lil.nlp.cornell.edu/newsroom/download/index.html) and then follow the link in your email to download. `test.jsonl` is in the downloaded tar ball. 
  - Execute `newsroom_build.sh`. 
* TAC: We assume that you have fully recursively extracted the two files. 
  - [`GuidedSumm2010_eval.tgz`](https://tac.nist.gov/protected/past-aquaint-aquaint2/2010/GuidedSumm2010_eval.tgz
) Downloadable from web, containing human evaluation results and system summaries. 
  - `TAC2010_Summarization_Documents.tgz` Emailed by NIST, containing the documents for which summaries are generated and rated. 
  Both files require you to apply to NIST for access. 

### Add your new metrics into the test##
Just add your score name in `env.py` and add a new corresponding function in `eval_util.py`'s `model_eval` function. 

### GPU

There are memory leaks when using GPUs. So, please distable GPUs: 
```shell
export CUDA_VISIBLE_DEVICES=''
```

## File structures/functions
* `env.py`: configurations of experimental settings
* `eval.py`: scoring summaries using automated metrics and computing their correlations with human scores
  - `eval_summary_level`: the main function that does summary-level evaluation. It loads a `dataset_df` (see specifications below)./
* `newsroom.py`: Run experiments on Newsroom dataset


## Key `Pandas.DataFrame`s in `eval.py`
We use the same local variable names for key DataFrames across functions in `eval.py` to be consistent. 
Please read more details of such variables in the docstrings/comments inside `eval.py` 

1. `dataset_df`: A summary human evaluation dataset represented as a `pandas.DataFrame`. 
   The following columns must be there: 
  - ArticleText -- the source text
  - System -- the name of the system/summarizer
  - SystemSummary -- a system summary generated by the `System` at the same row
  - ReferenceSummary -- corresponding reference summary provided by the dataset 
  - Various columns of human evaluation aspects, as defined in `env.human_metrics`

   2. `batch_result_df`: Each row corresponds to a sample and each column corresponds to a MultiIndex `(approach, model, score_name)`. `score_name` is a variant of a `model`, for example, ROUGE-1 is a `score_name` for ROUGE which is a `model`. 
3. `corr_df`: The correlation coefficient dataframe, the last column which is like 



